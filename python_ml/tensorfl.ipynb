{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tensorflow introduction from handson mechine learning\n",
    "import tensorflow as tf\n",
    "import tensorboard as tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensor flow basic operations\n",
    "#constants are entities who values can't change \n",
    "const_15 = tf.constant(15.0,name='constant_15.0')\n",
    "const_11 = tf.constant(11.0,name='constant_11.0')\n",
    "const_60 = tf.constant(60.0,name='constant_60')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Addition 26.0\n",
      "Substraction 4.0\n",
      "Total_sum 364.0\n",
      "Divison 4.0\n"
     ]
    }
   ],
   "source": [
    "#basic operations\n",
    "addition = tf.add(const_11,const_15,name = 'Adding_constants')\n",
    "Substraction = tf.subtract(const_15,const_11,name='Substraction_constants')\n",
    "Multiplication = tf.multiply(const_15,const_11,name='Mutlipy_constanst')\n",
    "Divison=tf.div(const_60,const_15,name='Division_constants')\n",
    "Total_sum = tf.add_n([addition,Substraction,Multiplication,Multiplication,Divison],name='Total')\n",
    "\n",
    "with tf.Session() as ses:\n",
    "    print('Addition',ses.run(addition))\n",
    "    print('Substraction',ses.run(Substraction))\n",
    "    print('Total_sum',ses.run(Total_sum))\n",
    "    print('Divison',ses.run(Divison))\n",
    "    \n",
    "#train_writer = tb.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.Variable(3,name='x')\n",
    "y = tf.Variable(4,name='y')\n",
    "f = x**2*y + x +2\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(x.initializer)\n",
    "sess.run(y.initializer)\n",
    "rslt = sess.run(f)\n",
    "rslt\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#in above example we have create session and close manually \n",
    "#and use always sess\n",
    "with tf.Session() as sess:\n",
    "    x.initializer.run()\n",
    "    y.initializer.run()\n",
    "    result= f.eval()\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** we can initialize variable as globally**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    result = f.eval();\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing()\n",
    "m,n = housing.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "housing_data_puls_bias = np.c_[np.ones((m,1)),housing.data]\n",
    "housing_data_puls_bias.shape\n",
    "housing_data_puls_bias.size\n",
    "housing_data_puls_bias[1:2,0]\n",
    "#print(housing_data_puls_bias)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  with tf.Session() as ses:\n",
    "#     print('housing_price',ses.run('housing_data_puls_bias'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = tf.constant(housing_data_puls_bias,dtype=tf.float32,name='X')\n",
    "y = tf.constant(housing.target.reshape(-1,1),dtype=tf.float32,name='y')\n",
    "X\n",
    "y\n",
    "a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XT = tf.transpose(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = tf.matmul(tf.matmul(tf.matrix_inverse(tf.matmul(XT,X)),XT),y)\n",
    "theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    theta_values = theta.eval()\n",
    "theta_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the necessary libraries\n",
    "import tensorflow as ft\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing data set\n",
    "data = keras.datasets.fashion_mnist\n",
    "#Load the data set and divide into training and test sets\n",
    "(train_image,train_lables),(test_image,test_lables) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out put label \n",
    "class_name = ['T-shirt/top','Trouser','Pullover',\n",
    "              'Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle boot']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scaling the values between o to 1\n",
    "train_image = train_image/255.0 # scale down the image\n",
    "test_image = test_image/255.0\n",
    "\n",
    "print(train_image[7])\n",
    "#displaying the image with pixel have black and white spots \n",
    "plt.imshow(train_image[7],cmap=plt.cm.binary)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make ready the nueral nework model and input the data set to model\n",
    "    model = keras.Sequential([\n",
    "     keras.layers.Flatten(input_shape=(28,28)),\n",
    "     keras.layers.Dense(128, activation='relu'),\n",
    "     keras.layers.Dense(10, activation='softmax')  \n",
    " ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#send the data to set for testing\n",
    "model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the data to the model \n",
    "model.fit(train_image, train_lables, epochs=5)\n",
    "test_loss , test_acc = model.evaluate(train_image,train_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('test acc:',test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(train_image, train_lables, epochs=10)\n",
    "test_loss , test_acc = model.evaluate(train_image,train_lables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now this time to predict images\n",
    "#input the prediction in the form fo 28*28 or np.array or test_images\n",
    "prediction = model.predict([test_image])\n",
    "print(prediction)\n",
    "# the prediction will print prabaility of prediction all \n",
    "#all test images from 10 nuerons which of getting highest values \n",
    "# We have to consider that is what looking predicted dress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the 10 class_name is the\n",
    "prediction = model.predict([test_image])\n",
    "print(prediction[0])\n",
    "print(prediction[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print the max prapability from the prediction\n",
    "print(class_name[np.argmax(prediction[1])])\n",
    "# argmax will print he maximum value in array\n",
    "#indexing the predicted values with class_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "#    plt.grid(False)\n",
    "    plt.imshow(test_image[i],cmap=plt.cm.binary)\n",
    "    plt.xlabel('Actual: ' + class_name[test_lables[i]])\n",
    "    plt.ylabel('Prediction:  ' + class_name[np.argmax(prediction[i])])\n",
    "    plt.show()\n",
    "print(class_name[np.argmax(prediction[i])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prediction = model.predict([test_image[7]])\n",
    "#print(class_name[np.arange(prediction)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text classification with movie reviews \n",
    "======================================\n",
    "\n",
    "use movie review data base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "from tensorflow import keras\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.imdb\n",
    "#(train_data, train_labels),(test_data,test_labels) = data.load_data(num_words=10000)\n",
    "(train_data, train_labels),(test_data,test_labels) = data.load_data(num_words=89000)\n",
    "print(train_data[0])\n",
    "print(train_labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = keras.datasets.imdb\n",
    "# # save np.load\n",
    "# np_load_old = np.load\n",
    "\n",
    "# # modify the default parameters of np.load\n",
    "# np.load = lambda *a,**k: np_load_old(*a,**k,allow_pickle=True)\n",
    "\n",
    "# # call load_data with allow_pickle implicitly set to true\n",
    "# #(train_data, train_labels), (test_data, test_labels) = data.load_data(num_words=10000)\n",
    "# (train_data, train_labels), (test_data, test_labels) = data.load_data(num_words=88000)\n",
    "\n",
    "# # restore np.load for future normal usage\n",
    "# np.load = np_load_old\n",
    "\n",
    "# print(train_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = data.get_word_index()\n",
    "word_index = {k:(v + 3) for k, v in word_index.items()}\n",
    "word_index['<PAD>'] = 0\n",
    "word_index['<START>'] = 1\n",
    "word_index['<UNK>'] = 2 # unkown character\n",
    "word_index['<UNUSE>'] = 3\n",
    "reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n",
    "\n",
    "\n",
    "#preprocessing data \n",
    "train_data = keras.preprocessing.sequence.pad_sequences(train_data, value=word_index['<PAD>'],padding='post',maxlen=250)\n",
    "test_data = keras.preprocessing.sequence.pad_sequences(test_data, value=word_index['<PAD>'],padding='post',maxlen=250)\n",
    "\n",
    "def decode_review(text):\n",
    "    return \"\".join([reverse_word_index.get(i, \"?\") for i in text])\n",
    "\n",
    "print(decode_review(test_data[0]))\n",
    "print('\\n')\n",
    "print(decode_review(test_data[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Model down hereb\n",
    "model = keras.Sequential()\n",
    "model.add(keras.layers.Embedding(89000,16))\n",
    "model.add(keras.layers.GlobalAveragePooling1D())\n",
    "model.add(keras.layers.Dense(16, activation='relu'))\n",
    "model.add(keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compile the model\n",
    "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "#validating the model\n",
    "x_val = train_data[:10000]\n",
    "x_train = train_data[10000:]\n",
    "\n",
    "y_val = train_labels[:10000]\n",
    "y_train = train_labels[10000:]\n",
    "\n",
    "fitModel = model.fit(x_train, y_train,epochs=40,batch_size=512,validation_data=(x_val, y_val), verbose=1)\n",
    "results = model.evaluate(test_data, test_labels)\n",
    "print(results)\n",
    "model.save('model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_review = test_data[0]\n",
    "predict = model.predict([test_review])\n",
    "print('Review: ')\n",
    "print(decode_review(test_review))\n",
    "print(\"Predictions: \" + str(predict[0]))\n",
    "print(\"Actucal: \" + str(test_labels[0]))\n",
    "print(results)\n",
    "print('\\n', 'second review')\n",
    "print(\"Predictions: \" + str(predict[0]))\n",
    "print(\"Actucal: \" + str(test_labels[0]))\n",
    "\n",
    "print('\\n', 'third review')\n",
    "print(\"Predictions: \" + str(predict[1]))\n",
    "print(\"Actucal: \" + str(test_labels[1]))\n",
    "\n",
    "print('\\n', 'last review')\n",
    "print(\"Predictions: \" + str(predict[2]))\n",
    "print(\"Actucal: \" + str(test_labels[2]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def review_encode(s):\n",
    "    encoded= [1]\n",
    "    \n",
    "    for word in s:\n",
    "        if word.lower() in word_index:\n",
    "            encoded.append(word_index[word.lower()])\n",
    "        else:\n",
    "            encoded.append(2)\n",
    "    return encoded\n",
    "\n",
    "new_model = keras.models.load_model('model.h5')\n",
    "\n",
    "with open('review_10.txt',encoding='utf-8') as f:\n",
    "    for line in f.readlines():\n",
    "        nline = line.replace(',', '').replace('.', '').replace('(','').replace(')', '').replace(':','').replace('!','').replace('\\'','').strip().split(' ')\n",
    "        encode = review_encode(nline)\n",
    "        encode = keras.preprocessing.sequence.pad_sequences([encode], value=word_index['<PAD>'],padding='post',maxlen=250)\n",
    "        predict = new_model.predict(encode)\n",
    "        print(line)\n",
    "        print(encode)\n",
    "        print(predict[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AI chatbot by techwithtim\n",
    "=========================\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "words = []\n",
    "labels = []\n",
    "docs_x = []\n",
    "docs_y = []\n",
    "\n",
    "for intent in data[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "        docs_x.append(wrds)\n",
    "        docs_y.append(intent[\"tag\"])\n",
    "\n",
    "    if intent[\"tag\"] not in labels:\n",
    "        labels.append(intent[\"tag\"])\n",
    "\n",
    "words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "labels = sorted(labels)\n",
    "\n",
    "training = []\n",
    "output = []\n",
    "\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "for x, doc in enumerate(docs_x):\n",
    "    bag = []\n",
    "\n",
    "    wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "    for w in words:\n",
    "        if w in wrds:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    output_row = out_empty[:]\n",
    "    output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "    training.append(bag)\n",
    "    output.append(output_row)\n",
    "\n",
    "\n",
    "training = numpy.array(training)\n",
    "output = numpy.array(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save(\"model.tflearn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
