{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### its explained chatbot\n",
    "from nltk import LancasterStemmer\n",
    "#nltk.download('punkt')\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Libraries needed for tensor flow processing \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import our chat-bot intents file\n",
    "with open('intents (2).json') as json_data:\n",
    "    intents = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2---words ['Hi']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?', 'Do', 'you', 'deliver', 'the', 'food', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?', 'Do', 'you', 'deliver', 'the', 'food', '?', 'What', 'are', 'the', 'home', 'delivery', 'options', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?', 'Do', 'you', 'deliver', 'the', 'food', '?', 'What', 'are', 'the', 'home', 'delivery', 'options', '?', 'What', 'is', 'your', 'Menu', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?', 'Do', 'you', 'deliver', 'the', 'food', '?', 'What', 'are', 'the', 'home', 'delivery', 'options', '?', 'What', 'is', 'your', 'Menu', '?', 'What', 'are', 'the', 'main', 'course', 'options', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?', 'Do', 'you', 'deliver', 'the', 'food', '?', 'What', 'are', 'the', 'home', 'delivery', 'options', '?', 'What', 'is', 'your', 'Menu', '?', 'What', 'are', 'the', 'main', 'course', 'options', '?', 'Can', 'you', 'tell', 'me', 'the', 'most', 'delicious', 'dish', 'from', 'the', 'menu', '?']\n",
      "\n",
      "\n",
      "2---words ['Hi', 'How', 'are', 'you', '?', '?', 'Is', 'anyone', 'there', '?', 'Hello', 'Good', 'day', 'Bye', 'See', 'you', 'later', 'Goodbye', 'Thanks', 'Thank', 'you', 'That', \"'s\", 'helpful', 'What', 'hours', 'are', 'you', 'open', '?', 'What', 'are', 'your', 'hours', '?', 'When', 'are', 'you', 'open', '?', 'What', 'is', 'your', 'location', '?', 'Where', 'are', 'you', 'located', '?', 'What', 'is', 'your', 'address', '?', 'Where', 'is', 'your', 'restaurant', 'situated', '?', 'Do', 'you', 'take', 'credit', 'cards', '?', 'Do', 'you', 'accept', 'Mastercard', '?', 'Are', 'you', 'cash', 'only', '?', 'What', 'is', 'your', 'menu', 'for', 'today', '?', 'What', 'are', 'you', 'serving', 'today', '?', 'What', 'is', 'today', \"'s\", 'special', '?', 'Do', 'you', 'provide', 'home', 'delivery', '?', 'Do', 'you', 'deliver', 'the', 'food', '?', 'What', 'are', 'the', 'home', 'delivery', 'options', '?', 'What', 'is', 'your', 'Menu', '?', 'What', 'are', 'the', 'main', 'course', 'options', '?', 'Can', 'you', 'tell', 'me', 'the', 'most', 'delicious', 'dish', 'from', 'the', 'menu', '?', 'What', 'is', 'the', 'today', \"'s\", 'special', '?']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#storing the words\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore = ['?'] # checking the punchuavtion \n",
    "#loop through each sentacne in the intens's parttern\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        #tokenize each and every word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "#         print('1____w', w)\n",
    "#         print('\\n')\n",
    "        #add word to the words list\n",
    "        words.extend(w)\n",
    "        print('2---words',words)\n",
    "        print('\\n')\n",
    "        #add word(s) to documents \n",
    "        documents.append((w, intent['tag']))\n",
    "       # print('3______',documents)\n",
    "    # inititally classes is empty one\n",
    "        if intent['tag'] not in classes:\n",
    "    # here intent is only value of the json file\n",
    "            classes.append(intent['tag'])\n",
    "         #   print('\\n',classes)\n",
    "            \n",
    "\n",
    "#my_list = [1,23,4,5,'hari','ranman']\n",
    "# my_list.extend('rupakumari') # adding items from iterable\n",
    "# my_list.append('ramanachari')# adding item at end of list\n",
    "# my_list\n",
    "#for intent in intents['intents']: \n",
    "#main intents.json file has dictionary and it have key and value pair\n",
    "#if want access the values you have to the key of particular key\n",
    "#intent: { value} because of this we have to use \n",
    "#intent['intent']\n",
    "#example:\n",
    "#my_dict = {\n",
    "#    'name': 'hari',\n",
    "#    'age': 22,\n",
    "#    'profession': 'doctor'   \n",
    "#}\n",
    "#my_dict['name']\n",
    "#for intent in intents[]:\n",
    "#    print(intent)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cours', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'resta', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n",
      "\n",
      "\n",
      "[\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cour', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'rest', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n",
      "31 documents\n",
      "9 classes\n",
      "57 Unique stemmed words [\"'s\", 'acceiv', 'address', 'anyon', 'ar', 'bye', 'can', 'card', 'cash', 'cour', 'credit', 'day', 'del', 'delicy', 'delivery', 'dish', 'do', 'food', 'for', 'from', 'good', 'goodby', 'hello', 'help', 'hi', 'hom', 'hour', 'how', 'is', 'lat', 'loc', 'main', 'mastercard', 'me', 'menu', 'most', 'on', 'op', 'opt', 'provid', 'rest', 'see', 'serv', 'situ', 'spec', 'tak', 'tel', 'thank', 'that', 'the', 'ther', 'today', 'what', 'when', 'wher', 'yo', 'you']\n"
     ]
    }
   ],
   "source": [
    "#Perfoming stemming and lower each word as well as remove duplicates\n",
    "print(words)\n",
    "print('\\n')\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
    "words = sorted(list(set(words)))\n",
    "print(words)\n",
    "#Remove duplicate classes\n",
    "classes = sorted(list(set(classes)))\n",
    "print(len(documents), 'documents')\n",
    "print(len(classes), 'classes')\n",
    "print(len(words), 'Unique stemmed words', words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'classes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-ae70651e8dfb>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#creating an empty for output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0moutput_empty\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclasses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'classes' is not defined"
     ]
    }
   ],
   "source": [
    "#Creating training data\n",
    "training = []\n",
    "output = []\n",
    "#creating an empty for output\n",
    "output_empty = [0] * len(classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Below code written behalf of the techwithtim youtube channel\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = [] # tokenize the patterns as per the nltk library\n",
    "labels = [] # labels have store the tag's\n",
    "docs_x = [] # like reading the line in patterns\n",
    "docs_y = [] # pattens corresponing tags\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        wrds = nltk.word_tokenize(pattern)\n",
    "        words.extend(wrds)\n",
    "       # print(wrds)\n",
    "        docs_x.append(pattern)\n",
    "        docs_y.append(intent['tag'])\n",
    "        #print(docs)\n",
    "        \n",
    "#find the tag's in the intents and append it to labels\n",
    "    if intent['tag'] not in labels:\n",
    "        labels.append(intent['tag'])\n",
    "        #print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hi', 'how', 'ar', 'you', 'is', 'anyon', 'ther', '?', 'hello', 'good', 'day', 'what', 'up', 'cya', 'see', 'you', 'lat', 'goodby', 'i', 'am', 'leav', 'hav', 'a', 'good', 'day', 'how', 'old', 'how', 'old', 'is', 'tim', 'what', 'is', 'yo', 'ag', 'how', 'old', 'ar', 'you', 'ag', '?', 'what', 'is', 'yo', 'nam', 'what', 'should', 'i', 'cal', 'you', 'what', 'yo', 'nam', '?', 'id', 'lik', 'to', 'buy', 'someth', 'what', 'on', 'the', 'menu', 'what', 'do', 'you', 'reccommend', '?', 'could', 'i', 'get', 'someth', 'to', 'eat', 'when', 'ar', 'you', 'guy', 'op', 'what', 'ar', 'yo', 'hour', 'hour', 'of', 'op']\n",
      "['?', 'a', 'ag', 'am', 'anyon', 'ar', 'buy', 'cal', 'could', 'cya', 'day', 'do', 'eat', 'get', 'good', 'goodby', 'guy', 'hav', 'hello', 'hi', 'hour', 'how', 'i', 'id', 'is', 'lat', 'leav', 'lik', 'menu', 'nam', 'of', 'old', 'on', 'op', 'reccommend', 'see', 'should', 'someth', 'the', 'ther', 'tim', 'to', 'up', 'what', 'when', 'yo', 'you']\n",
      "['age', 'goodbye', 'greeting', 'hours', 'name', 'shop']\n"
     ]
    }
   ],
   "source": [
    "words = [stemmer.stem(w.lower()) for w in words]\n",
    "print(words)\n",
    "words = sorted(list(set(words)))\n",
    "print(words)\n",
    "labels = sorted(labels)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]]\n",
      "[[0, 0, 0, 1, 0, 0]]\n"
     ]
    }
   ],
   "source": [
    "# train the dataset\n",
    "training = []\n",
    "output = []\n",
    "out_empty = [0 for _ in range(len(labels))]\n",
    "#print('out_empty---',out_empty)\n",
    "for x, docs in enumerate(docs_x):\n",
    "    bag = []\n",
    "    wrds = [stemmer.stem(w) for w in docs if w != '?']\n",
    "    #print(wrds)\n",
    "for w in wrds:\n",
    "    if w in wrds:\n",
    "        bag.append(1)\n",
    "    else:\n",
    "        bag.append(0)\n",
    "output_row = out_empty[:]\n",
    "output_row[labels.index(docs_y[x])] = 1\n",
    "print(output)\n",
    "training.append(bag)\n",
    "print(training)\n",
    "output.append(output_row)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n",
      "[[0 0 0 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "training = np.array(training)\n",
    "print(training)\n",
    "output = np.array(output)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pickle', 'wb') as f:\n",
    "    pickle.dump((words, labels,training,output), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\installation_files\\python_instl\\lib\\site-packages\\tflearn\\objectives.py:66: calling reduce_sum_v1 (from tensorflow.python.ops.math_ops) with keep_dims is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "keep_dims is deprecated, use keepdims instead\n",
      "WARNING:tensorflow:From E:\\installation_files\\python_instl\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "---------------------------------\n",
      "Run id: FAWOEO\n",
      "Log directory: /tmp/tflearn_logs/\n",
      "INFO:tensorflow:Summary name Accuracy/ (raw) is illegal; using Accuracy/__raw_ instead.\n",
      "---------------------------------\n",
      "Training samples: 1\n",
      "Validation samples: 0\n",
      "--\n",
      "Training Step: 1  | time: 1.781s\n",
      "| Adam | epoch: 001 | loss: 0.00000 - acc: 0.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 2  | total loss: \u001b[1m\u001b[32m1.61255\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 002 | loss: 1.61255 - acc: 0.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 3  | total loss: \u001b[1m\u001b[32m1.75757\u001b[0m\u001b[0m | time: 0.015s\n",
      "| Adam | epoch: 003 | loss: 1.75757 - acc: 0.9545 -- iter: 1/1\n",
      "--\n",
      "Training Step: 4  | total loss: \u001b[1m\u001b[32m1.78029\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 004 | loss: 1.78029 - acc: 0.9860 -- iter: 1/1\n",
      "--\n",
      "Training Step: 5  | total loss: \u001b[1m\u001b[32m1.78414\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 005 | loss: 1.78414 - acc: 0.9860 -- iter: 1/1\n",
      "--\n",
      "Training Step: 6  | total loss: \u001b[1m\u001b[32m1.78391\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 006 | loss: 1.78391 - acc: 0.9950 -- iter: 1/1\n",
      "--\n",
      "Training Step: 7  | total loss: \u001b[1m\u001b[32m1.78252\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 007 | loss: 1.78252 - acc: 0.9991 -- iter: 1/1\n",
      "--\n",
      "Training Step: 8  | total loss: \u001b[1m\u001b[32m1.78072\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 008 | loss: 1.78072 - acc: 0.9996 -- iter: 1/1\n",
      "--\n",
      "Training Step: 9  | total loss: \u001b[1m\u001b[32m1.77871\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 009 | loss: 1.77871 - acc: 0.9996 -- iter: 1/1\n",
      "--\n",
      "Training Step: 10  | total loss: \u001b[1m\u001b[32m1.77655\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 010 | loss: 1.77655 - acc: 0.9998 -- iter: 1/1\n",
      "--\n",
      "Training Step: 11  | total loss: \u001b[1m\u001b[32m1.77425\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 011 | loss: 1.77425 - acc: 0.9999 -- iter: 1/1\n",
      "--\n",
      "Training Step: 12  | total loss: \u001b[1m\u001b[32m1.77181\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 012 | loss: 1.77181 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 13  | total loss: \u001b[1m\u001b[32m1.76923\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 013 | loss: 1.76923 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 14  | total loss: \u001b[1m\u001b[32m1.76649\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 014 | loss: 1.76649 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 15  | total loss: \u001b[1m\u001b[32m1.76358\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 015 | loss: 1.76358 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 16  | total loss: \u001b[1m\u001b[32m1.76048\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 016 | loss: 1.76048 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 17  | total loss: \u001b[1m\u001b[32m1.75717\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 017 | loss: 1.75717 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 18  | total loss: \u001b[1m\u001b[32m1.75364\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 018 | loss: 1.75364 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 19  | total loss: \u001b[1m\u001b[32m1.74988\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 019 | loss: 1.74988 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 20  | total loss: \u001b[1m\u001b[32m1.74585\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 020 | loss: 1.74585 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 21  | total loss: \u001b[1m\u001b[32m1.74155\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 021 | loss: 1.74155 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 22  | total loss: \u001b[1m\u001b[32m1.73694\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 022 | loss: 1.73694 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 23  | total loss: \u001b[1m\u001b[32m1.73201\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 023 | loss: 1.73201 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 24  | total loss: \u001b[1m\u001b[32m1.72672\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 024 | loss: 1.72672 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 25  | total loss: \u001b[1m\u001b[32m1.72107\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 025 | loss: 1.72107 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 26  | total loss: \u001b[1m\u001b[32m1.71502\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 026 | loss: 1.71502 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 27  | total loss: \u001b[1m\u001b[32m1.70855\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 027 | loss: 1.70855 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 28  | total loss: \u001b[1m\u001b[32m1.70163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 028 | loss: 1.70163 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 29  | total loss: \u001b[1m\u001b[32m1.69422\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 029 | loss: 1.69422 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 30  | total loss: \u001b[1m\u001b[32m1.68632\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 030 | loss: 1.68632 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 31  | total loss: \u001b[1m\u001b[32m1.67787\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 031 | loss: 1.67787 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 32  | total loss: \u001b[1m\u001b[32m1.66887\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 032 | loss: 1.66887 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 33  | total loss: \u001b[1m\u001b[32m1.65927\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 033 | loss: 1.65927 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 34  | total loss: \u001b[1m\u001b[32m1.64904\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 034 | loss: 1.64904 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 35  | total loss: \u001b[1m\u001b[32m1.63816\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 035 | loss: 1.63816 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 36  | total loss: \u001b[1m\u001b[32m1.62659\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 036 | loss: 1.62659 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 37  | total loss: \u001b[1m\u001b[32m1.61430\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 037 | loss: 1.61430 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 38  | total loss: \u001b[1m\u001b[32m1.60126\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 038 | loss: 1.60126 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 39  | total loss: \u001b[1m\u001b[32m1.58745\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 039 | loss: 1.58745 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 40  | total loss: \u001b[1m\u001b[32m1.57283\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 040 | loss: 1.57283 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 41  | total loss: \u001b[1m\u001b[32m1.55737\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 041 | loss: 1.55737 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 42  | total loss: \u001b[1m\u001b[32m1.54105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 042 | loss: 1.54105 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 43  | total loss: \u001b[1m\u001b[32m1.52383\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 043 | loss: 1.52383 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 44  | total loss: \u001b[1m\u001b[32m1.50569\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 044 | loss: 1.50569 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 45  | total loss: \u001b[1m\u001b[32m1.48662\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 045 | loss: 1.48662 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 46  | total loss: \u001b[1m\u001b[32m1.46658\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 046 | loss: 1.46658 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 47  | total loss: \u001b[1m\u001b[32m1.44556\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 047 | loss: 1.44556 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 48  | total loss: \u001b[1m\u001b[32m1.42354\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 048 | loss: 1.42354 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 49  | total loss: \u001b[1m\u001b[32m1.40051\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 049 | loss: 1.40051 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 50  | total loss: \u001b[1m\u001b[32m1.37647\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 050 | loss: 1.37647 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 51  | total loss: \u001b[1m\u001b[32m1.35140\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 051 | loss: 1.35140 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 52  | total loss: \u001b[1m\u001b[32m1.32531\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 052 | loss: 1.32531 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 53  | total loss: \u001b[1m\u001b[32m1.29821\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 053 | loss: 1.29821 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 54  | total loss: \u001b[1m\u001b[32m1.27011\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 054 | loss: 1.27011 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 55  | total loss: \u001b[1m\u001b[32m1.24102\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 055 | loss: 1.24102 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 56  | total loss: \u001b[1m\u001b[32m1.21098\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 056 | loss: 1.21098 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 57  | total loss: \u001b[1m\u001b[32m1.18002\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 057 | loss: 1.18002 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 58  | total loss: \u001b[1m\u001b[32m1.14818\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 058 | loss: 1.14818 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 59  | total loss: \u001b[1m\u001b[32m1.11552\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 059 | loss: 1.11552 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 60  | total loss: \u001b[1m\u001b[32m1.08210\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 060 | loss: 1.08210 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 61  | total loss: \u001b[1m\u001b[32m1.04798\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 061 | loss: 1.04798 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 62  | total loss: \u001b[1m\u001b[32m1.01325\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 062 | loss: 1.01325 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 63  | total loss: \u001b[1m\u001b[32m0.97801\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 063 | loss: 0.97801 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 64  | total loss: \u001b[1m\u001b[32m0.94234\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 064 | loss: 0.94234 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 65  | total loss: \u001b[1m\u001b[32m0.90636\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 065 | loss: 0.90636 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 66  | total loss: \u001b[1m\u001b[32m0.87019\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 066 | loss: 0.87019 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 67  | total loss: \u001b[1m\u001b[32m0.83395\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 067 | loss: 0.83395 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 68  | total loss: \u001b[1m\u001b[32m0.79776\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 068 | loss: 0.79776 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 69  | total loss: \u001b[1m\u001b[32m0.76176\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 069 | loss: 0.76176 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 70  | total loss: \u001b[1m\u001b[32m0.72609\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 070 | loss: 0.72609 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 71  | total loss: \u001b[1m\u001b[32m0.69086\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 071 | loss: 0.69086 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 72  | total loss: \u001b[1m\u001b[32m0.65622\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 072 | loss: 0.65622 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 73  | total loss: \u001b[1m\u001b[32m0.62228\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 073 | loss: 0.62228 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 74  | total loss: \u001b[1m\u001b[32m0.58916\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 074 | loss: 0.58916 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 75  | total loss: \u001b[1m\u001b[32m0.55696\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 075 | loss: 0.55696 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 76  | total loss: \u001b[1m\u001b[32m0.52578\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 076 | loss: 0.52578 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 77  | total loss: \u001b[1m\u001b[32m0.49569\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 077 | loss: 0.49569 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 78  | total loss: \u001b[1m\u001b[32m0.46675\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 078 | loss: 0.46675 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 79  | total loss: \u001b[1m\u001b[32m0.43902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 079 | loss: 0.43902 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 80  | total loss: \u001b[1m\u001b[32m0.41254\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 080 | loss: 0.41254 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 81  | total loss: \u001b[1m\u001b[32m0.38732\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 081 | loss: 0.38732 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 82  | total loss: \u001b[1m\u001b[32m0.36337\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 082 | loss: 0.36337 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 83  | total loss: \u001b[1m\u001b[32m0.34043\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 083 | loss: 0.34043 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 84  | total loss: \u001b[1m\u001b[32m0.29775\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 084 | loss: 0.29775 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 85  | total loss: \u001b[1m\u001b[32m0.29775\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 085 | loss: 0.29775 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 86  | total loss: \u001b[1m\u001b[32m0.27803\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 086 | loss: 0.27803 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 87  | total loss: \u001b[1m\u001b[32m0.25939\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 087 | loss: 0.25939 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 88  | total loss: \u001b[1m\u001b[32m0.24182\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 088 | loss: 0.24182 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 89  | total loss: \u001b[1m\u001b[32m0.22529\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 089 | loss: 0.22529 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 90  | total loss: \u001b[1m\u001b[32m0.20979\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 090 | loss: 0.20979 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 91  | total loss: \u001b[1m\u001b[32m0.19526\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 091 | loss: 0.19526 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 92  | total loss: \u001b[1m\u001b[32m0.18169\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 092 | loss: 0.18169 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 93  | total loss: \u001b[1m\u001b[32m0.16902\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 093 | loss: 0.16902 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 94  | total loss: \u001b[1m\u001b[32m0.15721\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 094 | loss: 0.15721 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 95  | total loss: \u001b[1m\u001b[32m0.14621\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 095 | loss: 0.14621 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 96  | total loss: \u001b[1m\u001b[32m0.13600\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 096 | loss: 0.13600 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 97  | total loss: \u001b[1m\u001b[32m0.12651\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 097 | loss: 0.12651 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 98  | total loss: \u001b[1m\u001b[32m0.11770\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 098 | loss: 0.11770 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 99  | total loss: \u001b[1m\u001b[32m0.10954\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 099 | loss: 0.10954 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 100  | total loss: \u001b[1m\u001b[32m0.10198\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 100 | loss: 0.10198 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 101  | total loss: \u001b[1m\u001b[32m0.09498\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 101 | loss: 0.09498 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 102  | total loss: \u001b[1m\u001b[32m0.08850\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 102 | loss: 0.08850 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 103  | total loss: \u001b[1m\u001b[32m0.08251\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 103 | loss: 0.08251 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 104  | total loss: \u001b[1m\u001b[32m0.07698\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 104 | loss: 0.07698 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 105  | total loss: \u001b[1m\u001b[32m0.07186\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 105 | loss: 0.07186 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 106  | total loss: \u001b[1m\u001b[32m0.06713\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 106 | loss: 0.06713 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 107  | total loss: \u001b[1m\u001b[32m0.06277\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 107 | loss: 0.06277 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 108  | total loss: \u001b[1m\u001b[32m0.05873\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 108 | loss: 0.05873 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 109  | total loss: \u001b[1m\u001b[32m0.05501\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 109 | loss: 0.05501 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 110  | total loss: \u001b[1m\u001b[32m0.05157\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 110 | loss: 0.05157 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 111  | total loss: \u001b[1m\u001b[32m0.04840\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 111 | loss: 0.04840 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 112  | total loss: \u001b[1m\u001b[32m0.04546\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 112 | loss: 0.04546 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 113  | total loss: \u001b[1m\u001b[32m0.04276\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 113 | loss: 0.04276 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 114  | total loss: \u001b[1m\u001b[32m0.04025\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 114 | loss: 0.04025 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 115  | total loss: \u001b[1m\u001b[32m0.03794\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 115 | loss: 0.03794 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 116  | total loss: \u001b[1m\u001b[32m0.03581\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 116 | loss: 0.03581 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 117  | total loss: \u001b[1m\u001b[32m0.03383\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 117 | loss: 0.03383 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 118  | total loss: \u001b[1m\u001b[32m0.03201\u001b[0m\u001b[0m | time: 0.022s\n",
      "| Adam | epoch: 118 | loss: 0.03201 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 119  | total loss: \u001b[1m\u001b[32m0.03032\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 119 | loss: 0.03032 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 120  | total loss: \u001b[1m\u001b[32m0.02875\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 120 | loss: 0.02875 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 121  | total loss: \u001b[1m\u001b[32m0.02730\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 121 | loss: 0.02730 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 122  | total loss: \u001b[1m\u001b[32m0.02596\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 122 | loss: 0.02596 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 123  | total loss: \u001b[1m\u001b[32m0.02472\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 123 | loss: 0.02472 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 124  | total loss: \u001b[1m\u001b[32m0.02356\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 124 | loss: 0.02356 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 125  | total loss: \u001b[1m\u001b[32m0.02249\u001b[0m\u001b[0m | time: 0.007s\n",
      "| Adam | epoch: 125 | loss: 0.02249 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 126  | total loss: \u001b[1m\u001b[32m0.02150\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 126 | loss: 0.02150 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 127  | total loss: \u001b[1m\u001b[32m0.02057\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 127 | loss: 0.02057 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 128  | total loss: \u001b[1m\u001b[32m0.01971\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 128 | loss: 0.01971 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 129  | total loss: \u001b[1m\u001b[32m0.01891\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 129 | loss: 0.01891 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 130  | total loss: \u001b[1m\u001b[32m0.01817\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 130 | loss: 0.01817 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 131  | total loss: \u001b[1m\u001b[32m0.01747\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 131 | loss: 0.01747 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 132  | total loss: \u001b[1m\u001b[32m0.01682\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 132 | loss: 0.01682 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 133  | total loss: \u001b[1m\u001b[32m0.01622\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 133 | loss: 0.01622 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 134  | total loss: \u001b[1m\u001b[32m0.01565\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 134 | loss: 0.01565 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 135  | total loss: \u001b[1m\u001b[32m0.01512\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 135 | loss: 0.01512 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 136  | total loss: \u001b[1m\u001b[32m0.01462\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 136 | loss: 0.01462 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 137  | total loss: \u001b[1m\u001b[32m0.01415\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 137 | loss: 0.01415 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 138  | total loss: \u001b[1m\u001b[32m0.01372\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 138 | loss: 0.01372 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 139  | total loss: \u001b[1m\u001b[32m0.01330\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 139 | loss: 0.01330 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 140  | total loss: \u001b[1m\u001b[32m0.01292\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 140 | loss: 0.01292 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 141  | total loss: \u001b[1m\u001b[32m0.01255\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 141 | loss: 0.01255 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 142  | total loss: \u001b[1m\u001b[32m0.01221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 142 | loss: 0.01221 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 143  | total loss: \u001b[1m\u001b[32m0.01188\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 143 | loss: 0.01188 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 144  | total loss: \u001b[1m\u001b[32m0.01157\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 144 | loss: 0.01157 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 145  | total loss: \u001b[1m\u001b[32m0.01128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 145 | loss: 0.01128 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 146  | total loss: \u001b[1m\u001b[32m0.01101\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 146 | loss: 0.01101 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 147  | total loss: \u001b[1m\u001b[32m0.01074\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 147 | loss: 0.01074 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 148  | total loss: \u001b[1m\u001b[32m0.01049\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 148 | loss: 0.01049 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 149  | total loss: \u001b[1m\u001b[32m0.01026\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 149 | loss: 0.01026 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 150  | total loss: \u001b[1m\u001b[32m0.01003\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 150 | loss: 0.01003 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 151  | total loss: \u001b[1m\u001b[32m0.00982\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 151 | loss: 0.00982 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 152  | total loss: \u001b[1m\u001b[32m0.00961\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 152 | loss: 0.00961 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 153  | total loss: \u001b[1m\u001b[32m0.00942\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 153 | loss: 0.00942 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 154  | total loss: \u001b[1m\u001b[32m0.00923\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 154 | loss: 0.00923 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 155  | total loss: \u001b[1m\u001b[32m0.00888\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 155 | loss: 0.00888 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 156  | total loss: \u001b[1m\u001b[32m0.00888\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 156 | loss: 0.00888 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 157  | total loss: \u001b[1m\u001b[32m0.00871\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 157 | loss: 0.00871 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 158  | total loss: \u001b[1m\u001b[32m0.00855\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 158 | loss: 0.00855 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 159  | total loss: \u001b[1m\u001b[32m0.00840\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 159 | loss: 0.00840 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 160  | total loss: \u001b[1m\u001b[32m0.00825\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 160 | loss: 0.00825 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 161  | total loss: \u001b[1m\u001b[32m0.00811\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 161 | loss: 0.00811 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 162  | total loss: \u001b[1m\u001b[32m0.00797\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 162 | loss: 0.00797 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 163  | total loss: \u001b[1m\u001b[32m0.00784\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 163 | loss: 0.00784 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 164  | total loss: \u001b[1m\u001b[32m0.00772\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 164 | loss: 0.00772 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 165  | total loss: \u001b[1m\u001b[32m0.00759\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 165 | loss: 0.00759 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 166  | total loss: \u001b[1m\u001b[32m0.00747\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 166 | loss: 0.00747 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 167  | total loss: \u001b[1m\u001b[32m0.00736\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 167 | loss: 0.00736 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 168  | total loss: \u001b[1m\u001b[32m0.00725\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 168 | loss: 0.00725 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 169  | total loss: \u001b[1m\u001b[32m0.00714\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 169 | loss: 0.00714 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 170  | total loss: \u001b[1m\u001b[32m0.00703\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 170 | loss: 0.00703 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 171  | total loss: \u001b[1m\u001b[32m0.00693\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 171 | loss: 0.00693 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 172  | total loss: \u001b[1m\u001b[32m0.00683\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 172 | loss: 0.00683 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 173  | total loss: \u001b[1m\u001b[32m0.00673\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 173 | loss: 0.00673 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 174  | total loss: \u001b[1m\u001b[32m0.00664\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 174 | loss: 0.00664 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 175  | total loss: \u001b[1m\u001b[32m0.00655\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 175 | loss: 0.00655 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 176  | total loss: \u001b[1m\u001b[32m0.00646\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 176 | loss: 0.00646 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 177  | total loss: \u001b[1m\u001b[32m0.00637\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 177 | loss: 0.00637 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 178  | total loss: \u001b[1m\u001b[32m0.00629\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 178 | loss: 0.00629 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 179  | total loss: \u001b[1m\u001b[32m0.00621\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 179 | loss: 0.00621 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 180  | total loss: \u001b[1m\u001b[32m0.00613\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 180 | loss: 0.00613 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 181  | total loss: \u001b[1m\u001b[32m0.00605\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 181 | loss: 0.00605 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 182  | total loss: \u001b[1m\u001b[32m0.00597\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 182 | loss: 0.00597 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 183  | total loss: \u001b[1m\u001b[32m0.00590\u001b[0m\u001b[0m | time: 0.029s\n",
      "| Adam | epoch: 183 | loss: 0.00590 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 184  | total loss: \u001b[1m\u001b[32m0.00582\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 184 | loss: 0.00582 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 185  | total loss: \u001b[1m\u001b[32m0.00575\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 185 | loss: 0.00575 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 186  | total loss: \u001b[1m\u001b[32m0.00568\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 186 | loss: 0.00568 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 187  | total loss: \u001b[1m\u001b[32m0.00561\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 187 | loss: 0.00561 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 188  | total loss: \u001b[1m\u001b[32m0.00554\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 188 | loss: 0.00554 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 189  | total loss: \u001b[1m\u001b[32m0.00548\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 189 | loss: 0.00548 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 190  | total loss: \u001b[1m\u001b[32m0.00541\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 190 | loss: 0.00541 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 191  | total loss: \u001b[1m\u001b[32m0.00535\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 191 | loss: 0.00535 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 192  | total loss: \u001b[1m\u001b[32m0.00529\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 192 | loss: 0.00529 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 193  | total loss: \u001b[1m\u001b[32m0.00523\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 193 | loss: 0.00523 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 194  | total loss: \u001b[1m\u001b[32m0.00517\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 194 | loss: 0.00517 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 195  | total loss: \u001b[1m\u001b[32m0.00511\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 195 | loss: 0.00511 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 196  | total loss: \u001b[1m\u001b[32m0.00505\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 196 | loss: 0.00505 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 197  | total loss: \u001b[1m\u001b[32m0.00500\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 197 | loss: 0.00500 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 198  | total loss: \u001b[1m\u001b[32m0.00494\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 198 | loss: 0.00494 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 199  | total loss: \u001b[1m\u001b[32m0.00489\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 199 | loss: 0.00489 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 200  | total loss: \u001b[1m\u001b[32m0.00483\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 200 | loss: 0.00483 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 201  | total loss: \u001b[1m\u001b[32m0.00478\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 201 | loss: 0.00478 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 202  | total loss: \u001b[1m\u001b[32m0.00473\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 202 | loss: 0.00473 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 203  | total loss: \u001b[1m\u001b[32m0.00468\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 203 | loss: 0.00468 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 204  | total loss: \u001b[1m\u001b[32m0.00463\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 204 | loss: 0.00463 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 205  | total loss: \u001b[1m\u001b[32m0.00458\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 205 | loss: 0.00458 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 206  | total loss: \u001b[1m\u001b[32m0.00454\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 206 | loss: 0.00454 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 207  | total loss: \u001b[1m\u001b[32m0.00449\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 207 | loss: 0.00449 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 208  | total loss: \u001b[1m\u001b[32m0.00444\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 208 | loss: 0.00444 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 209  | total loss: \u001b[1m\u001b[32m0.00440\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 209 | loss: 0.00440 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 210  | total loss: \u001b[1m\u001b[32m0.00435\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 210 | loss: 0.00435 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 211  | total loss: \u001b[1m\u001b[32m0.00431\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 211 | loss: 0.00431 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 212  | total loss: \u001b[1m\u001b[32m0.00426\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 212 | loss: 0.00426 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 213  | total loss: \u001b[1m\u001b[32m0.00422\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 213 | loss: 0.00422 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 214  | total loss: \u001b[1m\u001b[32m0.00418\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 214 | loss: 0.00418 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 215  | total loss: \u001b[1m\u001b[32m0.00414\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 215 | loss: 0.00414 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 216  | total loss: \u001b[1m\u001b[32m0.00410\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 216 | loss: 0.00410 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 217  | total loss: \u001b[1m\u001b[32m0.00406\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 217 | loss: 0.00406 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 218  | total loss: \u001b[1m\u001b[32m0.00402\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 218 | loss: 0.00402 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 219  | total loss: \u001b[1m\u001b[32m0.00398\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 219 | loss: 0.00398 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 220  | total loss: \u001b[1m\u001b[32m0.00394\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 220 | loss: 0.00394 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 221  | total loss: \u001b[1m\u001b[32m0.00391\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 221 | loss: 0.00391 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 222  | total loss: \u001b[1m\u001b[32m0.00387\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 222 | loss: 0.00387 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 223  | total loss: \u001b[1m\u001b[32m0.00383\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 223 | loss: 0.00383 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 224  | total loss: \u001b[1m\u001b[32m0.00380\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 224 | loss: 0.00380 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 225  | total loss: \u001b[1m\u001b[32m0.00376\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 225 | loss: 0.00376 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 226  | total loss: \u001b[1m\u001b[32m0.00373\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 226 | loss: 0.00373 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 227  | total loss: \u001b[1m\u001b[32m0.00369\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 227 | loss: 0.00369 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 228  | total loss: \u001b[1m\u001b[32m0.00366\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 228 | loss: 0.00366 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 229  | total loss: \u001b[1m\u001b[32m0.00363\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 229 | loss: 0.00363 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 230  | total loss: \u001b[1m\u001b[32m0.00359\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 230 | loss: 0.00359 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 231  | total loss: \u001b[1m\u001b[32m0.00356\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 231 | loss: 0.00356 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 232  | total loss: \u001b[1m\u001b[32m0.00353\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 232 | loss: 0.00353 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 233  | total loss: \u001b[1m\u001b[32m0.00350\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 233 | loss: 0.00350 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 234  | total loss: \u001b[1m\u001b[32m0.00347\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 234 | loss: 0.00347 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 235  | total loss: \u001b[1m\u001b[32m0.00344\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 235 | loss: 0.00344 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 236  | total loss: \u001b[1m\u001b[32m0.00341\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 236 | loss: 0.00341 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 237  | total loss: \u001b[1m\u001b[32m0.00338\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 237 | loss: 0.00338 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 238  | total loss: \u001b[1m\u001b[32m0.00335\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 238 | loss: 0.00335 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 239  | total loss: \u001b[1m\u001b[32m0.00332\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 239 | loss: 0.00332 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 240  | total loss: \u001b[1m\u001b[32m0.00329\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 240 | loss: 0.00329 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 241  | total loss: \u001b[1m\u001b[32m0.00326\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 241 | loss: 0.00326 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 242  | total loss: \u001b[1m\u001b[32m0.00323\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 242 | loss: 0.00323 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 243  | total loss: \u001b[1m\u001b[32m0.00321\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 243 | loss: 0.00321 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 244  | total loss: \u001b[1m\u001b[32m0.00318\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 244 | loss: 0.00318 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 245  | total loss: \u001b[1m\u001b[32m0.00315\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 245 | loss: 0.00315 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 246  | total loss: \u001b[1m\u001b[32m0.00313\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 246 | loss: 0.00313 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 247  | total loss: \u001b[1m\u001b[32m0.00310\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 247 | loss: 0.00310 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 248  | total loss: \u001b[1m\u001b[32m0.00308\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 248 | loss: 0.00308 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 249  | total loss: \u001b[1m\u001b[32m0.00305\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 249 | loss: 0.00305 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 250  | total loss: \u001b[1m\u001b[32m0.00303\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 250 | loss: 0.00303 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 251  | total loss: \u001b[1m\u001b[32m0.00300\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 251 | loss: 0.00300 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 252  | total loss: \u001b[1m\u001b[32m0.00298\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 252 | loss: 0.00298 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 253  | total loss: \u001b[1m\u001b[32m0.00295\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 253 | loss: 0.00295 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 254  | total loss: \u001b[1m\u001b[32m0.00293\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 254 | loss: 0.00293 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 255  | total loss: \u001b[1m\u001b[32m0.00291\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 255 | loss: 0.00291 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 256  | total loss: \u001b[1m\u001b[32m0.00288\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 256 | loss: 0.00288 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 257  | total loss: \u001b[1m\u001b[32m0.00286\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 257 | loss: 0.00286 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 258  | total loss: \u001b[1m\u001b[32m0.00284\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 258 | loss: 0.00284 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 259  | total loss: \u001b[1m\u001b[32m0.00282\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 259 | loss: 0.00282 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 260  | total loss: \u001b[1m\u001b[32m0.00279\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 260 | loss: 0.00279 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 261  | total loss: \u001b[1m\u001b[32m0.00277\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 261 | loss: 0.00277 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 262  | total loss: \u001b[1m\u001b[32m0.00275\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 262 | loss: 0.00275 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 263  | total loss: \u001b[1m\u001b[32m0.00273\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 263 | loss: 0.00273 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 264  | total loss: \u001b[1m\u001b[32m0.00271\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 264 | loss: 0.00271 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 265  | total loss: \u001b[1m\u001b[32m0.00269\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 265 | loss: 0.00269 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 266  | total loss: \u001b[1m\u001b[32m0.00267\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 266 | loss: 0.00267 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 267  | total loss: \u001b[1m\u001b[32m0.00265\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 267 | loss: 0.00265 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 268  | total loss: \u001b[1m\u001b[32m0.00263\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 268 | loss: 0.00263 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 269  | total loss: \u001b[1m\u001b[32m0.00261\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 269 | loss: 0.00261 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 270  | total loss: \u001b[1m\u001b[32m0.00259\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 270 | loss: 0.00259 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 271  | total loss: \u001b[1m\u001b[32m0.00257\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 271 | loss: 0.00257 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 272  | total loss: \u001b[1m\u001b[32m0.00255\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 272 | loss: 0.00255 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 273  | total loss: \u001b[1m\u001b[32m0.00253\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 273 | loss: 0.00253 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 274  | total loss: \u001b[1m\u001b[32m0.00251\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 274 | loss: 0.00251 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 275  | total loss: \u001b[1m\u001b[32m0.00249\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 275 | loss: 0.00249 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 276  | total loss: \u001b[1m\u001b[32m0.00248\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 276 | loss: 0.00248 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 277  | total loss: \u001b[1m\u001b[32m0.00246\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 277 | loss: 0.00246 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 278  | total loss: \u001b[1m\u001b[32m0.00244\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 278 | loss: 0.00244 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 279  | total loss: \u001b[1m\u001b[32m0.00242\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 279 | loss: 0.00242 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 280  | total loss: \u001b[1m\u001b[32m0.00240\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 280 | loss: 0.00240 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 281  | total loss: \u001b[1m\u001b[32m0.00239\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 281 | loss: 0.00239 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 282  | total loss: \u001b[1m\u001b[32m0.00237\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 282 | loss: 0.00237 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 283  | total loss: \u001b[1m\u001b[32m0.00235\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 283 | loss: 0.00235 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 284  | total loss: \u001b[1m\u001b[32m0.00234\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 284 | loss: 0.00234 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 285  | total loss: \u001b[1m\u001b[32m0.00232\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 285 | loss: 0.00232 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 286  | total loss: \u001b[1m\u001b[32m0.00230\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 286 | loss: 0.00230 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 287  | total loss: \u001b[1m\u001b[32m0.00229\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 287 | loss: 0.00229 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 288  | total loss: \u001b[1m\u001b[32m0.00227\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 288 | loss: 0.00227 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 289  | total loss: \u001b[1m\u001b[32m0.00226\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 289 | loss: 0.00226 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 290  | total loss: \u001b[1m\u001b[32m0.00224\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 290 | loss: 0.00224 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 291  | total loss: \u001b[1m\u001b[32m0.00223\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 291 | loss: 0.00223 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 292  | total loss: \u001b[1m\u001b[32m0.00221\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 292 | loss: 0.00221 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 293  | total loss: \u001b[1m\u001b[32m0.00220\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 293 | loss: 0.00220 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 294  | total loss: \u001b[1m\u001b[32m0.00218\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 294 | loss: 0.00218 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 295  | total loss: \u001b[1m\u001b[32m0.00217\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 295 | loss: 0.00217 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 296  | total loss: \u001b[1m\u001b[32m0.00215\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 296 | loss: 0.00215 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 297  | total loss: \u001b[1m\u001b[32m0.00214\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 297 | loss: 0.00214 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 298  | total loss: \u001b[1m\u001b[32m0.00212\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 298 | loss: 0.00212 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 299  | total loss: \u001b[1m\u001b[32m0.00211\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 299 | loss: 0.00211 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 300  | total loss: \u001b[1m\u001b[32m0.00209\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 300 | loss: 0.00209 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 301  | total loss: \u001b[1m\u001b[32m0.00208\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 301 | loss: 0.00208 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 302  | total loss: \u001b[1m\u001b[32m0.00207\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 302 | loss: 0.00207 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 303  | total loss: \u001b[1m\u001b[32m0.00205\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 303 | loss: 0.00205 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 304  | total loss: \u001b[1m\u001b[32m0.00204\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 304 | loss: 0.00204 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 305  | total loss: \u001b[1m\u001b[32m0.00203\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 305 | loss: 0.00203 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 306  | total loss: \u001b[1m\u001b[32m0.00201\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 306 | loss: 0.00201 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 307  | total loss: \u001b[1m\u001b[32m0.00200\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 307 | loss: 0.00200 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 308  | total loss: \u001b[1m\u001b[32m0.00199\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 308 | loss: 0.00199 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 309  | total loss: \u001b[1m\u001b[32m0.00198\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 309 | loss: 0.00198 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 310  | total loss: \u001b[1m\u001b[32m0.00196\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 310 | loss: 0.00196 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 311  | total loss: \u001b[1m\u001b[32m0.00195\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 311 | loss: 0.00195 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 312  | total loss: \u001b[1m\u001b[32m0.00194\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 312 | loss: 0.00194 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 313  | total loss: \u001b[1m\u001b[32m0.00193\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 313 | loss: 0.00193 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 314  | total loss: \u001b[1m\u001b[32m0.00191\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 314 | loss: 0.00191 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 315  | total loss: \u001b[1m\u001b[32m0.00190\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 315 | loss: 0.00190 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 316  | total loss: \u001b[1m\u001b[32m0.00189\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 316 | loss: 0.00189 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 317  | total loss: \u001b[1m\u001b[32m0.00188\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 317 | loss: 0.00188 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 318  | total loss: \u001b[1m\u001b[32m0.00187\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 318 | loss: 0.00187 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 319  | total loss: \u001b[1m\u001b[32m0.00185\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 319 | loss: 0.00185 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 320  | total loss: \u001b[1m\u001b[32m0.00184\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 320 | loss: 0.00184 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 321  | total loss: \u001b[1m\u001b[32m0.00183\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 321 | loss: 0.00183 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 322  | total loss: \u001b[1m\u001b[32m0.00182\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 322 | loss: 0.00182 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 323  | total loss: \u001b[1m\u001b[32m0.00181\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 323 | loss: 0.00181 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 324  | total loss: \u001b[1m\u001b[32m0.00180\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 324 | loss: 0.00180 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 325  | total loss: \u001b[1m\u001b[32m0.00179\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 325 | loss: 0.00179 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 326  | total loss: \u001b[1m\u001b[32m0.00178\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 326 | loss: 0.00178 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 327  | total loss: \u001b[1m\u001b[32m0.00177\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 327 | loss: 0.00177 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 328  | total loss: \u001b[1m\u001b[32m0.00176\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 328 | loss: 0.00176 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 329  | total loss: \u001b[1m\u001b[32m0.00174\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 329 | loss: 0.00174 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 330  | total loss: \u001b[1m\u001b[32m0.00173\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 330 | loss: 0.00173 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 331  | total loss: \u001b[1m\u001b[32m0.00172\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 331 | loss: 0.00172 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 332  | total loss: \u001b[1m\u001b[32m0.00171\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 332 | loss: 0.00171 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 333  | total loss: \u001b[1m\u001b[32m0.00170\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 333 | loss: 0.00170 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 334  | total loss: \u001b[1m\u001b[32m0.00169\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 334 | loss: 0.00169 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 335  | total loss: \u001b[1m\u001b[32m0.00168\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 335 | loss: 0.00168 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 336  | total loss: \u001b[1m\u001b[32m0.00167\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 336 | loss: 0.00167 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 337  | total loss: \u001b[1m\u001b[32m0.00166\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 337 | loss: 0.00166 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 338  | total loss: \u001b[1m\u001b[32m0.00165\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 338 | loss: 0.00165 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 339  | total loss: \u001b[1m\u001b[32m0.00165\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 339 | loss: 0.00165 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 340  | total loss: \u001b[1m\u001b[32m0.00164\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 340 | loss: 0.00164 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 341  | total loss: \u001b[1m\u001b[32m0.00163\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 341 | loss: 0.00163 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 342  | total loss: \u001b[1m\u001b[32m0.00162\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 342 | loss: 0.00162 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 343  | total loss: \u001b[1m\u001b[32m0.00161\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 343 | loss: 0.00161 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 344  | total loss: \u001b[1m\u001b[32m0.00160\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 344 | loss: 0.00160 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 345  | total loss: \u001b[1m\u001b[32m0.00159\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 345 | loss: 0.00159 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 346  | total loss: \u001b[1m\u001b[32m0.00158\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 346 | loss: 0.00158 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 347  | total loss: \u001b[1m\u001b[32m0.00157\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 347 | loss: 0.00157 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 348  | total loss: \u001b[1m\u001b[32m0.00156\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 348 | loss: 0.00156 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 349  | total loss: \u001b[1m\u001b[32m0.00155\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 349 | loss: 0.00155 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 350  | total loss: \u001b[1m\u001b[32m0.00154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 350 | loss: 0.00154 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 351  | total loss: \u001b[1m\u001b[32m0.00154\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 351 | loss: 0.00154 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 352  | total loss: \u001b[1m\u001b[32m0.00153\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 352 | loss: 0.00153 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 353  | total loss: \u001b[1m\u001b[32m0.00152\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 353 | loss: 0.00152 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 354  | total loss: \u001b[1m\u001b[32m0.00151\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 354 | loss: 0.00151 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 355  | total loss: \u001b[1m\u001b[32m0.00150\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 355 | loss: 0.00150 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 356  | total loss: \u001b[1m\u001b[32m0.00149\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 356 | loss: 0.00149 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 357  | total loss: \u001b[1m\u001b[32m0.00149\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 357 | loss: 0.00149 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 358  | total loss: \u001b[1m\u001b[32m0.00148\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 358 | loss: 0.00148 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 359  | total loss: \u001b[1m\u001b[32m0.00147\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 359 | loss: 0.00147 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 360  | total loss: \u001b[1m\u001b[32m0.00146\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 360 | loss: 0.00146 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 361  | total loss: \u001b[1m\u001b[32m0.00145\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 361 | loss: 0.00145 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 362  | total loss: \u001b[1m\u001b[32m0.00145\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 362 | loss: 0.00145 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 363  | total loss: \u001b[1m\u001b[32m0.00144\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 363 | loss: 0.00144 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 364  | total loss: \u001b[1m\u001b[32m0.00143\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 364 | loss: 0.00143 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 365  | total loss: \u001b[1m\u001b[32m0.00142\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 365 | loss: 0.00142 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 366  | total loss: \u001b[1m\u001b[32m0.00142\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 366 | loss: 0.00142 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 367  | total loss: \u001b[1m\u001b[32m0.00141\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 367 | loss: 0.00141 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 368  | total loss: \u001b[1m\u001b[32m0.00140\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 368 | loss: 0.00140 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 369  | total loss: \u001b[1m\u001b[32m0.00139\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 369 | loss: 0.00139 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 370  | total loss: \u001b[1m\u001b[32m0.00139\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 370 | loss: 0.00139 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 371  | total loss: \u001b[1m\u001b[32m0.00138\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 371 | loss: 0.00138 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 372  | total loss: \u001b[1m\u001b[32m0.00137\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 372 | loss: 0.00137 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 373  | total loss: \u001b[1m\u001b[32m0.00136\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 373 | loss: 0.00136 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 374  | total loss: \u001b[1m\u001b[32m0.00136\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 374 | loss: 0.00136 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 375  | total loss: \u001b[1m\u001b[32m0.00135\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 375 | loss: 0.00135 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 376  | total loss: \u001b[1m\u001b[32m0.00134\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 376 | loss: 0.00134 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 377  | total loss: \u001b[1m\u001b[32m0.00134\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 377 | loss: 0.00134 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 378  | total loss: \u001b[1m\u001b[32m0.00133\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 378 | loss: 0.00133 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 379  | total loss: \u001b[1m\u001b[32m0.00132\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 379 | loss: 0.00132 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 380  | total loss: \u001b[1m\u001b[32m0.00132\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 380 | loss: 0.00132 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 381  | total loss: \u001b[1m\u001b[32m0.00131\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 381 | loss: 0.00131 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 382  | total loss: \u001b[1m\u001b[32m0.00130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 382 | loss: 0.00130 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 383  | total loss: \u001b[1m\u001b[32m0.00130\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 383 | loss: 0.00130 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 384  | total loss: \u001b[1m\u001b[32m0.00129\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 384 | loss: 0.00129 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 385  | total loss: \u001b[1m\u001b[32m0.00128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 385 | loss: 0.00128 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 386  | total loss: \u001b[1m\u001b[32m0.00128\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 386 | loss: 0.00128 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 387  | total loss: \u001b[1m\u001b[32m0.00127\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 387 | loss: 0.00127 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 388  | total loss: \u001b[1m\u001b[32m0.00126\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 388 | loss: 0.00126 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 389  | total loss: \u001b[1m\u001b[32m0.00126\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 389 | loss: 0.00126 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 390  | total loss: \u001b[1m\u001b[32m0.00125\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 390 | loss: 0.00125 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 391  | total loss: \u001b[1m\u001b[32m0.00124\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 391 | loss: 0.00124 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 392  | total loss: \u001b[1m\u001b[32m0.00124\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 392 | loss: 0.00124 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 393  | total loss: \u001b[1m\u001b[32m0.00123\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 393 | loss: 0.00123 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 394  | total loss: \u001b[1m\u001b[32m0.00123\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 394 | loss: 0.00123 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 395  | total loss: \u001b[1m\u001b[32m0.00122\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 395 | loss: 0.00122 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 396  | total loss: \u001b[1m\u001b[32m0.00121\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 396 | loss: 0.00121 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 397  | total loss: \u001b[1m\u001b[32m0.00121\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 397 | loss: 0.00121 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 398  | total loss: \u001b[1m\u001b[32m0.00120\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 398 | loss: 0.00120 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 399  | total loss: \u001b[1m\u001b[32m0.00120\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 399 | loss: 0.00120 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 400  | total loss: \u001b[1m\u001b[32m0.00119\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 400 | loss: 0.00119 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 401  | total loss: \u001b[1m\u001b[32m0.00118\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 401 | loss: 0.00118 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 402  | total loss: \u001b[1m\u001b[32m0.00118\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 402 | loss: 0.00118 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 403  | total loss: \u001b[1m\u001b[32m0.00117\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 403 | loss: 0.00117 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 404  | total loss: \u001b[1m\u001b[32m0.00117\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 404 | loss: 0.00117 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 405  | total loss: \u001b[1m\u001b[32m0.00116\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 405 | loss: 0.00116 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 406  | total loss: \u001b[1m\u001b[32m0.00116\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 406 | loss: 0.00116 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 407  | total loss: \u001b[1m\u001b[32m0.00115\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 407 | loss: 0.00115 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 408  | total loss: \u001b[1m\u001b[32m0.00115\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 408 | loss: 0.00115 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 409  | total loss: \u001b[1m\u001b[32m0.00114\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 409 | loss: 0.00114 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 410  | total loss: \u001b[1m\u001b[32m0.00113\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 410 | loss: 0.00113 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 411  | total loss: \u001b[1m\u001b[32m0.00113\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 411 | loss: 0.00113 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 412  | total loss: \u001b[1m\u001b[32m0.00112\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 412 | loss: 0.00112 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 413  | total loss: \u001b[1m\u001b[32m0.00112\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 413 | loss: 0.00112 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 414  | total loss: \u001b[1m\u001b[32m0.00111\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 414 | loss: 0.00111 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 415  | total loss: \u001b[1m\u001b[32m0.00111\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 415 | loss: 0.00111 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 416  | total loss: \u001b[1m\u001b[32m0.00110\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 416 | loss: 0.00110 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 417  | total loss: \u001b[1m\u001b[32m0.00110\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 417 | loss: 0.00110 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 418  | total loss: \u001b[1m\u001b[32m0.00109\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 418 | loss: 0.00109 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 419  | total loss: \u001b[1m\u001b[32m0.00109\u001b[0m\u001b[0m | time: 0.021s\n",
      "| Adam | epoch: 419 | loss: 0.00109 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 420  | total loss: \u001b[1m\u001b[32m0.00108\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 420 | loss: 0.00108 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 421  | total loss: \u001b[1m\u001b[32m0.00108\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 421 | loss: 0.00108 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 422  | total loss: \u001b[1m\u001b[32m0.00107\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 422 | loss: 0.00107 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 423  | total loss: \u001b[1m\u001b[32m0.00107\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 423 | loss: 0.00107 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 424  | total loss: \u001b[1m\u001b[32m0.00106\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 424 | loss: 0.00106 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 425  | total loss: \u001b[1m\u001b[32m0.00106\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 425 | loss: 0.00106 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 426  | total loss: \u001b[1m\u001b[32m0.00105\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 426 | loss: 0.00105 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 427  | total loss: \u001b[1m\u001b[32m0.00105\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 427 | loss: 0.00105 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 428  | total loss: \u001b[1m\u001b[32m0.00104\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 428 | loss: 0.00104 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 429  | total loss: \u001b[1m\u001b[32m0.00104\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 429 | loss: 0.00104 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 430  | total loss: \u001b[1m\u001b[32m0.00103\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 430 | loss: 0.00103 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 431  | total loss: \u001b[1m\u001b[32m0.00103\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 431 | loss: 0.00103 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 432  | total loss: \u001b[1m\u001b[32m0.00103\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 432 | loss: 0.00103 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 433  | total loss: \u001b[1m\u001b[32m0.00102\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 433 | loss: 0.00102 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 434  | total loss: \u001b[1m\u001b[32m0.00102\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 434 | loss: 0.00102 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 435  | total loss: \u001b[1m\u001b[32m0.00101\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 435 | loss: 0.00101 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 436  | total loss: \u001b[1m\u001b[32m0.00101\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 436 | loss: 0.00101 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 437  | total loss: \u001b[1m\u001b[32m0.00100\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 437 | loss: 0.00100 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 438  | total loss: \u001b[1m\u001b[32m0.00100\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 438 | loss: 0.00100 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 439  | total loss: \u001b[1m\u001b[32m0.00099\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 439 | loss: 0.00099 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 440  | total loss: \u001b[1m\u001b[32m0.00099\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 440 | loss: 0.00099 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 441  | total loss: \u001b[1m\u001b[32m0.00099\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 441 | loss: 0.00099 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 442  | total loss: \u001b[1m\u001b[32m0.00098\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 442 | loss: 0.00098 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 443  | total loss: \u001b[1m\u001b[32m0.00098\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 443 | loss: 0.00098 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 444  | total loss: \u001b[1m\u001b[32m0.00097\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 444 | loss: 0.00097 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 445  | total loss: \u001b[1m\u001b[32m0.00097\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 445 | loss: 0.00097 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 446  | total loss: \u001b[1m\u001b[32m0.00096\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 446 | loss: 0.00096 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 447  | total loss: \u001b[1m\u001b[32m0.00096\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 447 | loss: 0.00096 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 448  | total loss: \u001b[1m\u001b[32m0.00096\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 448 | loss: 0.00096 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 449  | total loss: \u001b[1m\u001b[32m0.00095\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 449 | loss: 0.00095 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 450  | total loss: \u001b[1m\u001b[32m0.00095\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 450 | loss: 0.00095 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 451  | total loss: \u001b[1m\u001b[32m0.00094\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 451 | loss: 0.00094 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 452  | total loss: \u001b[1m\u001b[32m0.00094\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 452 | loss: 0.00094 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 453  | total loss: \u001b[1m\u001b[32m0.00094\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 453 | loss: 0.00094 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 454  | total loss: \u001b[1m\u001b[32m0.00093\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 454 | loss: 0.00093 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 455  | total loss: \u001b[1m\u001b[32m0.00093\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 455 | loss: 0.00093 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 456  | total loss: \u001b[1m\u001b[32m0.00092\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 456 | loss: 0.00092 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 457  | total loss: \u001b[1m\u001b[32m0.00092\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 457 | loss: 0.00092 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 458  | total loss: \u001b[1m\u001b[32m0.00092\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 458 | loss: 0.00092 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 459  | total loss: \u001b[1m\u001b[32m0.00091\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 459 | loss: 0.00091 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 460  | total loss: \u001b[1m\u001b[32m0.00091\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 460 | loss: 0.00091 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 461  | total loss: \u001b[1m\u001b[32m0.00090\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 461 | loss: 0.00090 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 462  | total loss: \u001b[1m\u001b[32m0.00090\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 462 | loss: 0.00090 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 463  | total loss: \u001b[1m\u001b[32m0.00090\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 463 | loss: 0.00090 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 464  | total loss: \u001b[1m\u001b[32m0.00089\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 464 | loss: 0.00089 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 465  | total loss: \u001b[1m\u001b[32m0.00089\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 465 | loss: 0.00089 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 466  | total loss: \u001b[1m\u001b[32m0.00089\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 466 | loss: 0.00089 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 467  | total loss: \u001b[1m\u001b[32m0.00088\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 467 | loss: 0.00088 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 468  | total loss: \u001b[1m\u001b[32m0.00088\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 468 | loss: 0.00088 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 469  | total loss: \u001b[1m\u001b[32m0.00087\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 469 | loss: 0.00087 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 470  | total loss: \u001b[1m\u001b[32m0.00087\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 470 | loss: 0.00087 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 471  | total loss: \u001b[1m\u001b[32m0.00087\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 471 | loss: 0.00087 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 472  | total loss: \u001b[1m\u001b[32m0.00086\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 472 | loss: 0.00086 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 473  | total loss: \u001b[1m\u001b[32m0.00086\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 473 | loss: 0.00086 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 474  | total loss: \u001b[1m\u001b[32m0.00086\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 474 | loss: 0.00086 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 475  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.006s\n",
      "| Adam | epoch: 475 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 476  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 476 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 477  | total loss: \u001b[1m\u001b[32m0.00085\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 477 | loss: 0.00085 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 478  | total loss: \u001b[1m\u001b[32m0.00084\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 478 | loss: 0.00084 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 479  | total loss: \u001b[1m\u001b[32m0.00084\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 479 | loss: 0.00084 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 480  | total loss: \u001b[1m\u001b[32m0.00084\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 480 | loss: 0.00084 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 481  | total loss: \u001b[1m\u001b[32m0.00083\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 481 | loss: 0.00083 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 482  | total loss: \u001b[1m\u001b[32m0.00083\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 482 | loss: 0.00083 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 483  | total loss: \u001b[1m\u001b[32m0.00083\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 483 | loss: 0.00083 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 484  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 484 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 485  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 485 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 486  | total loss: \u001b[1m\u001b[32m0.00082\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 486 | loss: 0.00082 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 487  | total loss: \u001b[1m\u001b[32m0.00081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 487 | loss: 0.00081 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 488  | total loss: \u001b[1m\u001b[32m0.00081\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 488 | loss: 0.00081 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 489  | total loss: \u001b[1m\u001b[32m0.00081\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 489 | loss: 0.00081 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 490  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.002s\n",
      "| Adam | epoch: 490 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 491  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 491 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 492  | total loss: \u001b[1m\u001b[32m0.00080\u001b[0m\u001b[0m | time: 0.004s\n",
      "| Adam | epoch: 492 | loss: 0.00080 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 493  | total loss: \u001b[1m\u001b[32m0.00079\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 493 | loss: 0.00079 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 494  | total loss: \u001b[1m\u001b[32m0.00079\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 494 | loss: 0.00079 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 495  | total loss: \u001b[1m\u001b[32m0.00079\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 495 | loss: 0.00079 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 496  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m\n",
      "| Adam | epoch: 496 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 497  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 497 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 498  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.005s\n",
      "| Adam | epoch: 498 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 499  | total loss: \u001b[1m\u001b[32m0.00078\u001b[0m\u001b[0m | time: 0.003s\n",
      "| Adam | epoch: 499 | loss: 0.00078 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "Training Step: 500  | total loss: \u001b[1m\u001b[32m0.00077\u001b[0m\u001b[0m | time: 0.001s\n",
      "| Adam | epoch: 500 | loss: 0.00077 - acc: 1.0000 -- iter: 1/1\n",
      "--\n",
      "INFO:tensorflow:F:\\Python\\Python_scripts\\python_ml\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "#writing model\n",
    "tensorflow.reset_default_graph()\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "model.fit(training, output, n_epoch = 500, batch_size= 8, show_metric = True)\n",
    "model.save('model.tflearn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from F:\\Python\\Python_scripts\\python_ml\\model.tflearn\n",
      "Start talking with the bot (type quit to stop)!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  googby\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Cannot feed value of shape (1, 47) for Tensor 'InputData/X:0', which has shape '(?, 18)'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m--------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-02569b93dcbd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponses\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m \u001b[0mchat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m \u001b[1;31m# def bag_of_words(s, words):\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[1;31m#     bag = [ 0 for _ in range(len(words))]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-02569b93dcbd>\u001b[0m in \u001b[0;36mchat\u001b[1;34m()\u001b[0m\n\u001b[0;32m    103\u001b[0m             \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mresults_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[0mtag\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mresults_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation_files\\python_instl\\lib\\site-packages\\tflearn\\models\\dnn.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    255\u001b[0m         \"\"\"\n\u001b[0;32m    256\u001b[0m         \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfeed_dict_builder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 257\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredictor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpredict_label\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation_files\\python_instl\\lib\\site-packages\\tflearn\\helpers\\evaluator.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, feed_dict)\u001b[0m\n\u001b[0;32m     67\u001b[0m             \u001b[0mprediction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 69\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     70\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0moutput\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensors\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation_files\\python_instl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mE:\\installation_files\\python_instl\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1126\u001b[0m                              \u001b[1;34m'which has shape %r'\u001b[0m \u001b[1;33m%\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1127\u001b[0m                              (np_val.shape, subfeed_t.name,\n\u001b[1;32m-> 1128\u001b[1;33m                               str(subfeed_t.get_shape())))\n\u001b[0m\u001b[0;32m   1129\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_feedable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1130\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Tensor %s may not be fed.'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot feed value of shape (1, 47) for Tensor 'InputData/X:0', which has shape '(?, 18)'"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "with open(\"intents.json\") as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "try:\n",
    "    with open(\"data.pickle\", \"rb\") as f:\n",
    "        words, labels, training, output = pickle.load(f)\n",
    "except:\n",
    "    words = []\n",
    "    labels = []\n",
    "    docs_x = []\n",
    "    docs_y = []\n",
    "\n",
    "    for intent in data[\"intents\"]:\n",
    "        for pattern in intent[\"patterns\"]:\n",
    "            wrds = nltk.word_tokenize(pattern)\n",
    "            words.extend(wrds)\n",
    "            docs_x.append(wrds)\n",
    "            docs_y.append(intent[\"tag\"])\n",
    "\n",
    "        if intent[\"tag\"] not in labels:\n",
    "            labels.append(intent[\"tag\"])\n",
    "\n",
    "    words = [stemmer.stem(w.lower()) for w in words if w != \"?\"]\n",
    "    words = sorted(list(set(words)))\n",
    "\n",
    "    labels = sorted(labels)\n",
    "\n",
    "    training = []\n",
    "    output = []\n",
    "\n",
    "    out_empty = [0 for _ in range(len(labels))]\n",
    "\n",
    "    for x, doc in enumerate(docs_x):\n",
    "        bag = []\n",
    "\n",
    "        wrds = [stemmer.stem(w.lower()) for w in doc]\n",
    "\n",
    "        for w in words:\n",
    "            if w in wrds:\n",
    "                bag.append(1)\n",
    "            else:\n",
    "                bag.append(0)\n",
    "\n",
    "        output_row = out_empty[:]\n",
    "        output_row[labels.index(docs_y[x])] = 1\n",
    "\n",
    "        training.append(bag)\n",
    "        output.append(output_row)\n",
    "\n",
    "\n",
    "    training = numpy.array(training)\n",
    "    output = numpy.array(output)\n",
    "\n",
    "    with open(\"data.pickle\", \"wb\") as f:\n",
    "        pickle.dump((words, labels, training, output), f)\n",
    "\n",
    "tensorflow.reset_default_graph()\n",
    "\n",
    "net = tflearn.input_data(shape=[None, len(training[0])])\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, 8)\n",
    "net = tflearn.fully_connected(net, len(output[0]), activation=\"softmax\")\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "model = tflearn.DNN(net)\n",
    "\n",
    "try:\n",
    "    model.load(\"model.tflearn\")\n",
    "except:\n",
    "    model.fit(training, output, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "    model.save(\"model.tflearn\")\n",
    "\n",
    "def bag_of_words(s, words):\n",
    "    bag = [0 for _ in range(len(words))]\n",
    "\n",
    "    s_words = nltk.word_tokenize(s)\n",
    "    s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "\n",
    "    for se in s_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == se:\n",
    "                bag[i] = 1\n",
    "            \n",
    "    return numpy.array(bag)\n",
    "\n",
    "\n",
    "def chat():\n",
    "    print(\"Start talking with the bot (type quit to stop)!\")\n",
    "    while True:\n",
    "        inp = input(\"You: \")\n",
    "        if inp.lower() == \"quit\":\n",
    "            break\n",
    "\n",
    "        results = model.predict([bag_of_words(inp, words)])\n",
    "        results_index = numpy.argmax(results)\n",
    "        tag = labels[results_index]\n",
    "\n",
    "        for tg in data[\"intents\"]:\n",
    "            if tg['tag'] == tag:\n",
    "                responses = tg['responses']\n",
    "\n",
    "        print(random.choice(responses))\n",
    "\n",
    "chat()    \n",
    "# def bag_of_words(s, words):\n",
    "#     bag = [ 0 for _ in range(len(words))]\n",
    "#     s_words = nltk.word_tokenize(s)\n",
    "#     s_words = [stemmer.stem(word.lower()) for word in s_words]\n",
    "    \n",
    "#     for se in s_words:\n",
    "#          for i, w in enumerate(words):\n",
    "#                 if w == se:\n",
    "#                     bag[i] = 1\n",
    "                    \n",
    "#     return np.array(bag)\n",
    "\n",
    "# def chat():\n",
    "#     print('Start talking with the bot (type quit to stop. !)')\n",
    "#     while True:\n",
    "#         inp = input('you: ')\n",
    "#         if inp.lower() == 'quit':\n",
    "#             break\n",
    "#         result = model.predict([bag_of_words(inp, words)])\n",
    "#         print(result)\n",
    "#         result_index = np.argmax(result)\n",
    "#         tag = labels[results_index]\n",
    "#         print(tag)\n",
    "        \n",
    "#         for tg in data['intents']:\n",
    "#             if tg['tag'] == tag:\n",
    "#                 responses = tg['responses']\n",
    "#         print(random.choice(responses))\n",
    "# chat()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 3999  | total loss: \u001b[1m\u001b[32m0.11507\u001b[0m\u001b[0m | time: 0.008s\n",
      "| Adam | epoch: 1000 | loss: 0.11507 - acc: 0.9996 -- iter: 24/26\n",
      "Training Step: 4000  | total loss: \u001b[1m\u001b[32m0.11436\u001b[0m\u001b[0m | time: 0.010s\n",
      "| Adam | epoch: 1000 | loss: 0.11436 - acc: 0.9997 -- iter: 26/26\n",
      "--\n",
      "INFO:tensorflow:F:\\Python\\Python_scripts\\python_ml\\model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n",
      "WARNING:tensorflow:From E:\\installation_files\\python_instl\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from F:\\Python\\Python_scripts\\python_ml\\model.tflearn\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"Chatbot.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/11VdQtbFk2C9RqlkDiu_DgJNqxWSPUr9K\n",
    "\"\"\"\n",
    "\n",
    "# Libraries needed for NLP\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "stemmer = LancasterStemmer()\n",
    "\n",
    "# Libraries needed for Tensorflow processing\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tflearn\n",
    "import random\n",
    "import json\n",
    "\n",
    "# from google.colab import files\n",
    "# files.upload()\n",
    "\n",
    "# import our chat-bot intents file\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "intents\n",
    "\n",
    "words = []\n",
    "classes = []\n",
    "documents = []\n",
    "ignore = ['?']\n",
    "# loop through each sentence in the intent's patterns\n",
    "for intent in intents['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        # tokenize each and every word in the sentence\n",
    "        w = nltk.word_tokenize(pattern)\n",
    "        # add word to the words list\n",
    "        words.extend(w)\n",
    "        # add word(s) to documents\n",
    "        documents.append((w, intent['tag']))\n",
    "        # add tags to our classes list\n",
    "        if intent['tag'] not in classes:\n",
    "            classes.append(intent['tag'])\n",
    "\n",
    "# Perform stemming and lower each word as well as remove duplicates\n",
    "words = [stemmer.stem(w.lower()) for w in words if w not in ignore]\n",
    "words = sorted(list(set(words)))\n",
    "\n",
    "# remove duplicate classes\n",
    "classes = sorted(list(set(classes)))\n",
    "\n",
    "print (len(documents), \"documents\")\n",
    "print (len(classes), \"classes\", classes)\n",
    "print (len(words), \"unique stemmed words\", words)\n",
    "\n",
    "# create training data\n",
    "training = []\n",
    "output = []\n",
    "# create an empty array for output\n",
    "output_empty = [0] * len(classes)\n",
    "\n",
    "# create training set, bag of words for each sentence\n",
    "for doc in documents:\n",
    "    # initialize bag of words\n",
    "    bag = []\n",
    "    # list of tokenized words for the pattern\n",
    "    pattern_words = doc[0]\n",
    "    # stemming each word\n",
    "    pattern_words = [stemmer.stem(word.lower()) for word in pattern_words]\n",
    "    # create bag of words array\n",
    "    for w in words:\n",
    "        bag.append(1) if w in pattern_words else bag.append(0)\n",
    "\n",
    "    # output is '1' for current tag and '0' for rest of other tags\n",
    "    output_row = list(output_empty)\n",
    "    output_row[classes.index(doc[1])] = 1\n",
    "\n",
    "    training.append([bag, output_row])\n",
    "\n",
    "# shuffling features and turning it into np.array\n",
    "random.shuffle(training)\n",
    "training = np.array(training)\n",
    "\n",
    "# creating training lists\n",
    "train_x = list(training[:,0])\n",
    "train_y = list(training[:,1])\n",
    "\n",
    "# resetting underlying graph data\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Building neural network\n",
    "net = tflearn.input_data(shape=[None, len(train_x[0])])\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, 10)\n",
    "net = tflearn.fully_connected(net, len(train_y[0]), activation='softmax')\n",
    "net = tflearn.regression(net)\n",
    "\n",
    "# Defining model and setting up tensorboard\n",
    "model = tflearn.DNN(net, tensorboard_dir='tflearn_logs')\n",
    "\n",
    "# Start training\n",
    "model.fit(train_x, train_y, n_epoch=1000, batch_size=8, show_metric=True)\n",
    "model.save('model.tflearn')\n",
    "\n",
    "import pickle\n",
    "pickle.dump( {'words':words, 'classes':classes, 'train_x':train_x, 'train_y':train_y}, open( \"training_data\", \"wb\" ) )\n",
    "\n",
    "# restoring all the data structures\n",
    "data = pickle.load( open( \"training_data\", \"rb\" ) )\n",
    "words = data['words']\n",
    "classes = data['classes']\n",
    "train_x = data['train_x']\n",
    "train_y = data['train_y']\n",
    "\n",
    "with open('intents.json') as json_data:\n",
    "    intents = json.load(json_data)\n",
    "\n",
    "# load the saved model\n",
    "model.load('./model.tflearn')\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # tokenizing the pattern\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    # stemming each word\n",
    "    sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "# returning bag of words array: 0 or 1 for each word in the bag that exists in the sentence\n",
    "def bow(sentence, words, show_details=False):\n",
    "    # tokenizing the pattern\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    # generating bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))\n",
    "\n",
    "ERROR_THRESHOLD = 0.30\n",
    "def classify(sentence):\n",
    "    # generate probabilities from the model\n",
    "    results = model.predict([bow(sentence, words)])[0]\n",
    "    # filter out predictions below a threshold\n",
    "    results = [[i,r] for i,r in enumerate(results) if r>ERROR_THRESHOLD]\n",
    "    # sort by strength of probability\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append((classes[r[0]], r[1]))\n",
    "    # return tuple of intent and probability\n",
    "    return return_list\n",
    "\n",
    "def response(sentence, userID='123', show_details=False):\n",
    "    results = classify(sentence)\n",
    "    # if we have a classification then find the matching intent tag\n",
    "    if results:\n",
    "        # loop as long as there are matches to process\n",
    "        while results:\n",
    "            for i in intents['intents']:\n",
    "                # find a tag matching the first result\n",
    "                if i['tag'] == results[0][0]:\n",
    "                    # a random response from the intent\n",
    "                    return print(random.choice(i['responses']))\n",
    "\n",
    "            results.pop(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('greeting', 0.9198628)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "response('Can you please let me know the delivery options?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('greeting', 0.5938931), ('goodbye', 0.3049938)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classify('where you located: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "response('Do you accept Credit Card?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
